{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "{-# LANGUAGE FunctionalDependencies #-}\n",
    "{-# LANGUAGE MonomorphismRestriction #-}\n",
    "{-# LANGUAGE FlexibleContexts #-}\n",
    "{-# LANGUAGE ConstraintKinds #-}\n",
    "{-# LANGUAGE TemplateHaskell #-}\n",
    "{-# LANGUAGE OverloadedStrings #-}\n",
    "{-# LANGUAGE BangPatterns #-}\n",
    "\n",
    "import Control.Arrow\n",
    "import Control.Monad.State\n",
    "import Control.Monad.Trans.Class\n",
    "import Numeric.LinearAlgebra\n",
    "import Numeric.LinearAlgebra.Data\n",
    "import Data.List.Split\n",
    "import Data.List\n",
    "import Numeric\n",
    "import Prelude hiding ((<>))\n",
    "import Network.HTTP.Client\n",
    "import Network.HTTP.Types.Status (statusCode)\n",
    "import qualified Codec.Compression.GZip as GZip\n",
    "import System.Directory\n",
    "import System.FilePath.Posix\n",
    "import System.Random\n",
    "import qualified Data.ByteString.Lazy  as BS\n",
    "import qualified Data.ByteString as BSS\n",
    "import Data.Binary as B\n",
    "import Data.Binary.Get.Internal as BGI\n",
    "import Data.Time.Format\n",
    "import Data.Time.LocalTime\n",
    "import Debug.Trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "Sigmoid \\nonumber \\\\\n",
    "  h(x) &=& \\frac\n",
    "  {1}\n",
    "  {1 + \\exp(-x)} \\\\\n",
    "\\nonumber \\\\\n",
    "ReLU \\nonumber \\\\\n",
    "  h(x) &=& \\begin{cases}\n",
    "  x & (x > 0) \\\\\n",
    "  0 & (x \\le 0)\n",
    "  \\end{cases}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "sigmoid :: (Floating a) => a -> a\n",
    "sigmoid a = fromIntegral 1 / (fromIntegral 1 + exp (-a))\n",
    "\n",
    "sigmoidm :: (Floating a, Container Matrix a) => Matrix a -> Matrix a\n",
    "sigmoidm = cmap sigmoid\n",
    "\n",
    "sigmoidBackward :: (Floating a, Num (Vector a), Container Matrix a) => Matrix a -> Matrix a -> Matrix a\n",
    "sigmoidBackward y d = d * (cmap (\\a -> fromIntegral 1 - a) y) * y\n",
    "\n",
    "relu :: (Ord a, Num a) => a -> a\n",
    "relu = max $ fromIntegral 0\n",
    "\n",
    "relum :: (Ord a, Num a, Container Matrix a) => Matrix a -> Matrix a\n",
    "relum = cmap relu\n",
    "\n",
    "relumBackward :: (Ord a, Num a, Num (Matrix a), Container Vector a, Container Matrix a) => Matrix a -> Matrix a -> Matrix a\n",
    "relumBackward x d = d * mask\n",
    "    where mask = cmap (\\a -> fromIntegral $ if (0 < a) then 1 else 0) x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "Cross Entropy \\nonumber \\\\\n",
    "  L &=& -\\displaystyle \\sum_{i=1}^{n}t_i \\log (y_i + d) \\\\\n",
    "  t &:& 教師ラベル \\nonumber \\\\\n",
    "  d &:& 微小値 \\nonumber \\\\\n",
    " \\nonumber \\\\\n",
    "Softmax \\nonumber \\\\\n",
    "  y_k &=& \\frac\n",
    "  {\\exp(a_k - \\hat{a})}\n",
    "  {\\displaystyle \\sum_{i=1}^{n}\\exp(a_i - \\hat{a})} \\\\\n",
    "  \\hat{a} &=& \\max \\{ a_{1...n} \\} \\nonumber\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "softmax :: (Floating a, Container Vector a) => Vector a -> Vector a\n",
    "softmax v = cmap (/s) v'\n",
    "    where\n",
    "        m = maxElement v\n",
    "        v' = cmap (\\a -> exp (a - m)) v\n",
    "        s = sumElements v'\n",
    "\n",
    "softmaxm :: (Floating a, Container Vector a) => Matrix a -> Matrix a\n",
    "softmaxm m = fromRows $ map softmax $ toRows m\n",
    "\n",
    "crossEntropy :: (Floating a, Num (Vector a), Container Vector a) => Vector a -> Vector a -> a\n",
    "crossEntropy t y = -(sumElements $ cmap log (y + d) * t)\n",
    "    where d = 1e-10\n",
    "\n",
    "crossEntropym :: (Floating a, Num (Vector a), Container Vector a) => Matrix a -> Matrix a -> a\n",
    "crossEntropym t m = sum vs / batchSize\n",
    "    where \n",
    "        vs = uncurry crossEntropy `map` (toRows t `zip` toRows m)\n",
    "        batchSize = fromIntegral $ rows t\n",
    "\n",
    "softmaxWithCross :: (Floating a, Num (Vector a), Container Vector a) => Matrix a -> Matrix a -> a\n",
    "softmaxWithCross t = crossEntropym t . softmaxm\n",
    "\n",
    "softmaxWithCrossBackward :: (Floating a, Num (Vector a), Container Vector a) => Matrix a -> Matrix a -> Matrix a\n",
    "softmaxWithCrossBackward t y = (y - t) / batchSize\n",
    "    where batchSize = fromIntegral $ rows t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affinem :: (Floating a, Numeric a, Num (Vector a)) => Matrix a -> Vector a -> Matrix a -> Matrix a\n",
    "affinem w b x = x <> w + b'\n",
    "    where b' = fromRows $ replicate (rows x) b\n",
    "\n",
    "affinemBackward :: (Floating a, Numeric a) => Matrix a -> Matrix a -> Matrix a -> (Matrix a, Matrix a, Vector a)\n",
    "affinemBackward w x d = (dx, dw, db)\n",
    "    where\n",
    "        dx = d <> tr w\n",
    "        dw = tr x <> d\n",
    "        db = fromList $ sumElements `map` toColumns d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type Weight a = Matrix a\n",
    "type Bias a = Vector a\n",
    "type SignalX a = Matrix a\n",
    "type SignalY a = Matrix a\n",
    "type Diff a = Matrix a\n",
    "type TeacherBatch a = Matrix a\n",
    "type InputBatch a = Matrix a\n",
    "newtype TrainBatch a = TrainBatch (TeacherBatch a, InputBatch a) deriving (Show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data ForwardLayer a =\n",
    "    AffineForward (Weight a) (Bias a)\n",
    "  | SigmoidForward\n",
    "  | ReLUForward\n",
    "  | JoinedForwardLayer (ForwardLayer a) (ForwardLayer a)\n",
    "  deriving (Show)\n",
    "\n",
    "infixl 4 ~>\n",
    "(~>) :: ForwardLayer a -> ForwardLayer a -> ForwardLayer a\n",
    "a ~> (JoinedForwardLayer x y) = (a ~> x) ~> y\n",
    "a ~> b = JoinedForwardLayer a b\n",
    "\n",
    "data OutputLayer a = SoftmaxWithCrossForward\n",
    "  deriving (Show)\n",
    "\n",
    "data ForwardNN a = ForwardNN (ForwardLayer a) (OutputLayer a)\n",
    "  deriving (Show)\n",
    "\n",
    "data BackwardLayer a = \n",
    "    AffineBackward (Weight a) (Bias a) (SignalX a)\n",
    "  | SigmoidBackward (SignalY a)\n",
    "  | ReLUBackward (SignalX a)\n",
    "  | JoinedBackwardLayer (BackwardLayer a) (BackwardLayer a)\n",
    "  deriving (Show)\n",
    "\n",
    "infixr 4 <~\n",
    "(<~) :: BackwardLayer a -> BackwardLayer a -> BackwardLayer a\n",
    "(JoinedBackwardLayer x y) <~ b = x <~ (y <~ b)\n",
    "a <~ b = JoinedBackwardLayer a b\n",
    "\n",
    "data BackputLayer a = SoftmaxWithCrossBackward (TeacherBatch a) (SignalY a)\n",
    "\n",
    "data BackwardNN a = BackwardNN (BackwardLayer a) (BackputLayer a)\n",
    "\n",
    "type NElement a = (Ord a, Floating a, Numeric a, Num (Vector a), Show a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward :: NElement a => ForwardLayer a -> SignalX a -> (BackwardLayer a, SignalY a)\n",
    "forward (AffineForward w b) x = (AffineBackward w b x, affinem w b x)\n",
    "forward SigmoidForward x = let y = sigmoidm x in (SigmoidBackward y, y)\n",
    "forward ReLUForward x = (ReLUBackward x, relum x)\n",
    "forward (JoinedForwardLayer a b) x0 = (a' <~ b', x2)\n",
    "    where\n",
    "        (a', x1) = forward a x0\n",
    "        (b', x2) = forward b x1\n",
    "\n",
    "backward :: NElement a => BackwardLayer a -> Diff a -> (ForwardLayer a, Diff a)\n",
    "backward (AffineBackward w b x) d =  (AffineForward (w + w') (b + b'), x')\n",
    "    where (x', w', b') = affinemBackward w x d\n",
    "backward (SigmoidBackward y) d = (SigmoidForward, sigmoidBackward y d)\n",
    "backward (ReLUBackward x) d = (ReLUForward, relumBackward x d)\n",
    "backward (JoinedBackwardLayer a b) d0 = (a' ~> b', d2)\n",
    "    where\n",
    "        (b', d1) = backward b d0\n",
    "        (a', d2) = backward a d1\n",
    "\n",
    "output :: NElement a => OutputLayer a -> TeacherBatch a -> SignalY a -> (BackputLayer a, a)\n",
    "output SoftmaxWithCrossForward t y = (SoftmaxWithCrossBackward t y, softmaxWithCross t y)\n",
    "\n",
    "backput :: NElement a => BackputLayer a -> (OutputLayer a, Diff a)\n",
    "backput (SoftmaxWithCrossBackward t y) = (SoftmaxWithCrossForward, softmaxWithCrossBackward t y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnForward :: NElement a => ForwardNN a -> TrainBatch a -> (BackwardNN a, a)\n",
    "learnForward (ForwardNN layers loss) (TrainBatch (t, x)) = result `seq` (BackwardNN layers' loss', result)\n",
    "    where\n",
    "        (layers', y) = forward layers x\n",
    "        (loss', result) = output loss t y\n",
    "\n",
    "learnBackward :: NElement a => BackwardNN a -> ForwardNN a\n",
    "learnBackward  (BackwardNN layers loss) = ForwardNN layers' loss'\n",
    "    where\n",
    "        (loss', d) = backput loss\n",
    "        (layers', _) = backward layers d\n",
    "\n",
    "learn :: NElement a => ForwardNN a -> TrainBatch a -> (ForwardNN a, a)\n",
    "learn a = first learnBackward . learnForward a\n",
    "\n",
    "learnAll :: NElement a => ForwardNN a -> [TrainBatch a] -> (ForwardNN a, [a])\n",
    "learnAll origin = foldr f (origin, [])\n",
    "    where f batch (a, results) = trace (\"Learning iterate: \" ++ show (length results + 1)) $ a `seq` second (:results) $ learn a batch\n",
    "\n",
    "predict :: NElement a => ForwardLayer a -> Vector a -> Int\n",
    "predict layers = maxIndex . flatten . snd . (forward layers) . asRow\n",
    "\n",
    "evaluate :: NElement a => ForwardLayer a -> [(Int, Vector a)] -> Double\n",
    "evaluate layers samples = fromIntegral nOk / fromIntegral (length samples)\n",
    "    where\n",
    "        nOk = length $ filter (\\(a, b) -> a == b) results\n",
    "        results = map (second $ predict layers) samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "newtype MnistLabels = MnistLabels [Word8] deriving (Show)\n",
    "newtype MnistImages = MnistImages [Matrix Z] deriving (Show)\n",
    "newtype MnistData = MnistData [(Word8, Matrix Z)] deriving (Show)\n",
    "\n",
    "zipMnist :: MnistLabels -> MnistImages -> MnistData\n",
    "zipMnist (MnistLabels labels) (MnistImages images) = MnistData $ labels `zip` images\n",
    "\n",
    "markMinistLabels = 2049\n",
    "markMnistImages = 2051\n",
    "\n",
    "putAsWord32 :: Integral a => a -> Put\n",
    "putAsWord32 a = B.put (fromIntegral a :: Word32)\n",
    "\n",
    "getAsIntegral :: Integral a => Get a\n",
    "getAsIntegral = fromIntegral <$> (B.get :: Get Word32)\n",
    "\n",
    "instance B.Binary MnistLabels where\n",
    "    put (MnistLabels labels) = do\n",
    "        B.put markMinistLabels\n",
    "        putAsWord32 $ length labels\n",
    "        B.put $ BS.pack labels\n",
    "\n",
    "    get = do\n",
    "        mark <- getAsIntegral\n",
    "        guard $ mark == markMinistLabels\n",
    "        size <- getAsIntegral\n",
    "        let total = trace (\"Reading labels: \" ++ show size) size\n",
    "        labels <- BGI.readN total BSS.unpack\n",
    "        return $ MnistLabels $ labels\n",
    "\n",
    "instance B.Binary MnistImages where\n",
    "    put (MnistImages (images @ (hm:_))) = do\n",
    "        putAsWord32 markMnistImages\n",
    "        putAsWord32 $ length images\n",
    "        putAsWord32 $ rows hm\n",
    "        putAsWord32 $ cols hm\n",
    "        B.put $ (BS.pack . map fromIntegral . concat . concat . map toLists) images\n",
    "\n",
    "    get = do\n",
    "        mark <- getAsIntegral\n",
    "        guard $ mark == markMnistImages\n",
    "        size <- getAsIntegral\n",
    "        nRows <- getAsIntegral\n",
    "        nCols <- getAsIntegral\n",
    "        let total = trace (\"Reading images: \" ++ show size ++ \" of \" ++ show nRows ++ \"x\" ++ show nCols) size * nRows * nCols\n",
    "        zs <- map fromIntegral <$> BGI.readN total BSS.unpack\n",
    "        let images = map (\\a -> enRows >< nCols $! a) $ chunksOf (nRows * nCols) zs\n",
    "        return $ MnistImages $! images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "timestamp msg = do\n",
    "    t <- getZonedTime\n",
    "    let stamp = formatTime defaultTimeLocale \"[%Y/%m/%d %H:%M:%S] \" t\n",
    "    print $ stamp ++ msg\n",
    "\n",
    "download :: Manager -> String -> IO BS.ByteString\n",
    "download manager url = do\n",
    "   request <- parseRequest url\n",
    "   timestamp $ \"Downloading \" ++ url\n",
    "   response <- httpLbs request manager\n",
    "   return $ responseBody response\n",
    "\n",
    "saveMnist :: FilePath -> String -> [String] -> Manager -> IO [FilePath]\n",
    "saveMnist rootDir urlBase filenames manager = do\n",
    "    isDir <- doesDirectoryExist rootDir\n",
    "    _ <- if isDir then return () else createDirectory rootDir\n",
    "    mapM saveFile filenames\n",
    "    where\n",
    "        saveFile filename = do\n",
    "            let url = urlBase ++ filename\n",
    "            let file = rootDir </> filename\n",
    "            timestamp $ \"Checking file \" ++ file\n",
    "            e <- doesFileExist file\n",
    "            if e then return () else download manager url >>= (BS.writeFile file)\n",
    "            return file\n",
    "\n",
    "readMnist :: FilePath -> IO (Either MnistLabels MnistImages)\n",
    "readMnist file = do\n",
    "    timestamp $ \"decoding \" ++ file ++ \" ...\"\n",
    "    bs <- GZip.decompress <$> BS.readFile file\n",
    "    let a = left (const $ B.decode bs) $ right (\\(_, _, a) -> a) $ B.decodeOrFail bs\n",
    "    timestamp $ \"decoded \" ++ file\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "shuffleList :: Show a => [a] -> IO [a]\n",
    "shuffleList src =  doShuffle [] src <$> newStdGen\n",
    "    where\n",
    "        doShuffle rs [] g = rs\n",
    "        doShuffle rs (x:xs) g = doShuffle (ys ++ (trace \"Picking element\" x):zs) xs g'\n",
    "            where\n",
    "                (i, g') = randomR (0, length rs - 1) g\n",
    "                (ys, zs) = splitAt i rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "normalAffine :: Int -> Int -> IO (ForwardLayer R)\n",
    "normalAffine nIn nOut = do\n",
    "        weights <- rand nIn nOut\n",
    "        bias <- flatten <$> rand nOut 1\n",
    "        return $ (AffineForward weights bias) ~> ReLUForward\n",
    "\n",
    "initNN :: IO (ForwardNN R)\n",
    "initNN = do\n",
    "    affine1 <- normalAffine (28 * 28) 100\n",
    "    affine2 <- normalAffine 100 10\n",
    "    return $ ForwardNN (affine1 ~> affine2) SoftmaxWithCrossForward\n",
    "\n",
    "convertTrains :: Int -> MnistData -> [TrainBatch R]\n",
    "convertTrains batchSize (MnistData src) = map (mkTrainer . unzip) $ chunksOf batchSize $! vectors\n",
    "    where\n",
    "        vectors = map (first (\\a -> hotone 10 $! a) . second (\\a -> flatten $! a)) src\n",
    "        mkTrainer (a, b) = TrainBatch (fromRows a, 255 / (fromZ $ fromRows b))\n",
    "\n",
    "hotone :: (Integral v, NElement a) => Int -> v -> (Vector a)\n",
    "hotone n v = fromList $ map fromIntegral list\n",
    "    where\n",
    "        list = replicate i 0 ++ 1 : replicate (n - i - 1) 0\n",
    "        i = fromIntegral v\n",
    "\n",
    "convertTests :: MnistData -> [(Int, Vector R)]\n",
    "convertTests (MnistData src) = map (first fromIntegral . second ((255/) . flatten . fromZ)) src\n",
    "\n",
    "shuffleMnist :: MnistData -> IO MnistData\n",
    "shuffleMnist (MnistData list) = MnistData <$> return list\n",
    "\n",
    "trainingSimple :: ForwardNN R -> MnistData -> MnistData -> IO Double\n",
    "trainingSimple nn trainData testData = do\n",
    "    trainBatches <- convertTrains 100 <$> shuffleMnist trainData\n",
    "    let (ForwardNN layers _, _) = learnAll nn trainBatches\n",
    "    let rate = evaluate layers $ convertTests testData\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "newManager defaultManagerSettings >>= saveMnist \"mnist\" \"http://yann.lecun.com/exdb/mnist/\" [\n",
    "    \"train-images-idx3-ubyte.gz\"\n",
    "  , \"train-labels-idx1-ubyte.gz\"\n",
    "  , \"t10k-images-idx3-ubyte.gz\"\n",
    "  , \"t10k-labels-idx1-ubyte.gz\"\n",
    "  ] >>= mapM readMnist >>= \\ms -> do\n",
    "      let [trainers, tests] = map (\\[Right a, Left b] -> b `zipMnist` a) $ chunksOf 2 ms\n",
    "      origin <- initNN\n",
    "      result <- trainingSimple origin trainers tests\n",
    "      timestamp $ \"result=\" ++ show (result * 100) ++ \"%\"\n",
    "      return result"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Haskell",
   "language": "haskell",
   "name": "haskell"
  },
  "language_info": {
   "codemirror_mode": "ihaskell",
   "file_extension": ".hs",
   "name": "haskell",
   "pygments_lexer": "Haskell",
   "version": "8.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
