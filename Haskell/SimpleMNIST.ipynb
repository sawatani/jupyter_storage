{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "{-# LANGUAGE FunctionalDependencies #-}\n",
    "{-# LANGUAGE MonomorphismRestriction #-}\n",
    "{-# LANGUAGE FlexibleContexts #-}\n",
    "{-# LANGUAGE ConstraintKinds #-}\n",
    "{-# LANGUAGE TemplateHaskell #-}\n",
    "{-# LANGUAGE OverloadedStrings #-}\n",
    "{-# LANGUAGE BangPatterns #-}\n",
    "{-# LANGUAGE ScopedTypeVariables #-}\n",
    "\n",
    "import Control.Arrow\n",
    "import Control.DeepSeq\n",
    "import Control.Monad.State\n",
    "import Control.Monad.ST\n",
    "import Control.Monad.Trans.Class\n",
    "import Numeric.LinearAlgebra\n",
    "import Numeric.LinearAlgebra.Data\n",
    "import Data.List.Split\n",
    "import Data.List\n",
    "import Numeric\n",
    "import Prelude hiding ((<>))\n",
    "import Network.HTTP.Client\n",
    "import Network.HTTP.Types.Status (statusCode)\n",
    "import qualified Codec.Compression.GZip as GZip\n",
    "import System.Directory\n",
    "import System.FilePath.Posix\n",
    "import System.Random.MWC\n",
    "import qualified Data.ByteString.Lazy  as BS\n",
    "import qualified Data.ByteString as BSS\n",
    "import qualified Data.Vector as V\n",
    "import qualified Data.Vector.Generic as VG\n",
    "import qualified Data.Vector.Generic.Mutable as VM\n",
    "import Data.Binary as B\n",
    "import Data.Binary.Get.Internal as BGI\n",
    "import Data.Time.Clock\n",
    "import Data.Time.Format\n",
    "import Data.Time.LocalTime\n",
    "import Debug.Trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "Sigmoid \\nonumber \\\\\n",
    "  h(x) &=& \\frac\n",
    "  {1}\n",
    "  {1 + \\exp(-x)} \\\\\n",
    "\\nonumber \\\\\n",
    "ReLU \\nonumber \\\\\n",
    "  h(x) &=& \\begin{cases}\n",
    "  x & (x > 0) \\\\\n",
    "  0 & (x \\le 0)\n",
    "  \\end{cases}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "sigmoid :: (Floating a) => a -> a\n",
    "sigmoid a = fromIntegral 1 / (fromIntegral 1 + exp (-a))\n",
    "\n",
    "sigmoidm :: (Floating a, Container Matrix a) => Matrix a -> Matrix a\n",
    "sigmoidm = cmap sigmoid\n",
    "\n",
    "sigmoidBackward :: (Floating a, Num (Vector a), Container Matrix a) => Matrix a -> Matrix a -> Matrix a\n",
    "sigmoidBackward y d = d * (cmap (\\a -> fromIntegral 1 - a) y) * y\n",
    "\n",
    "relu :: (Ord a, Num a) => a -> a\n",
    "relu = max $ fromIntegral 0\n",
    "\n",
    "relum :: (Ord a, Num a, Container Matrix a) => Matrix a -> Matrix a\n",
    "relum = cmap relu\n",
    "\n",
    "relumBackward :: (Ord a, Num a, Num (Matrix a), Container Vector a, Container Matrix a) => Matrix a -> Matrix a -> Matrix a\n",
    "relumBackward x d = d * mask\n",
    "    where mask = cmap (\\a -> fromIntegral $ if (0 < a) then 1 else 0) x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "Cross Entropy \\nonumber \\\\\n",
    "  L &=& -\\displaystyle \\sum_{i=1}^{n}t_i \\log (y_i + d) \\\\\n",
    "  t &:& 教師ラベル \\nonumber \\\\\n",
    "  d &:& 微小値 \\nonumber \\\\\n",
    " \\nonumber \\\\\n",
    "Softmax \\nonumber \\\\\n",
    "  y_k &=& \\frac\n",
    "  {\\exp(a_k - \\hat{a})}\n",
    "  {\\displaystyle \\sum_{i=1}^{n}\\exp(a_i - \\hat{a})} \\\\\n",
    "  \\hat{a} &=& \\max \\{ a_{1...n} \\} \\nonumber\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "softmax :: (Floating a, Container Vector a) => Vector a -> Vector a\n",
    "softmax v = cmap (/s) v'\n",
    "    where\n",
    "        m = maxElement v\n",
    "        v' = cmap (\\a -> exp (a - m)) v\n",
    "        s = sumElements v'\n",
    "\n",
    "softmaxm :: (Floating a, Container Vector a) => Matrix a -> Matrix a\n",
    "softmaxm m = fromRows $ map softmax $ toRows m\n",
    "\n",
    "crossEntropy :: (Floating a, Num (Vector a), Container Vector a) => Vector a -> Vector a -> a\n",
    "crossEntropy t y = -(sumElements $ cmap log (y + d) * t)\n",
    "    where d = 1e-10\n",
    "\n",
    "crossEntropym :: (Floating a, Num (Vector a), Container Vector a) => Matrix a -> Matrix a -> a\n",
    "crossEntropym t m = sum vs / batchSize\n",
    "    where \n",
    "        vs = uncurry crossEntropy `map` (toRows t `zip` toRows m)\n",
    "        batchSize = fromIntegral $ rows t\n",
    "\n",
    "softmaxWithCross :: (Floating a, Num (Vector a), Container Vector a) => Matrix a -> Matrix a -> (Matrix a, a)\n",
    "softmaxWithCross t x = (y, crossEntropym t y)\n",
    "    where\n",
    "        y = softmaxm x\n",
    "\n",
    "softmaxWithCrossBackward :: (Floating a, Num (Vector a), Container Vector a, Show a) => Matrix a -> Matrix a -> Matrix a\n",
    "softmaxWithCrossBackward t y = (y - t) / batchSize\n",
    "    where\n",
    "        batchSize = fromIntegral $ rows t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affinem :: (Floating a, Numeric a, Num (Vector a), Show a) => Matrix a -> Vector a -> Matrix a -> Matrix a\n",
    "affinem w b x = trace (\"Affine result: \" ++ show (sumElements r)\n",
    "        ++ \"\\nAffile weight: \" ++ show (sumElements w)\n",
    "        ++ \"\\nAffine bias: \" ++ show (sumElements b')\n",
    "        ++ \"\\nAffine backword: \" ++ show (sumElements x)\n",
    "        ) r\n",
    "    where\n",
    "        r = (x <> w) + b'\n",
    "        b' = fromRows $ replicate (rows x) b\n",
    "\n",
    "affinemBackward :: (Floating a, Numeric a, Show a) => Matrix a -> Matrix a -> Matrix a -> (Matrix a, Matrix a, Vector a)\n",
    "affinemBackward w x d = (dx, dw, db)\n",
    "    where\n",
    "        dx = d <> tr w\n",
    "        dw = tr x <> d\n",
    "        db = fromList $ sumElements `map` toColumns d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type Weight a = Matrix a\n",
    "type Bias a = Vector a\n",
    "type SignalX a = Matrix a\n",
    "type SignalY a = Matrix a\n",
    "type Diff a = Matrix a\n",
    "type TeacherBatch a = Matrix a\n",
    "type InputBatch a = Matrix a\n",
    "newtype TrainBatch a = TrainBatch (TeacherBatch a, InputBatch a) deriving (Show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data ForwardLayer a =\n",
    "    AffineForward (Weight a) (Bias a)\n",
    "  | SigmoidForward\n",
    "  | ReLUForward\n",
    "  | JoinedForwardLayer (ForwardLayer a) (ForwardLayer a)\n",
    "  deriving (Show)\n",
    "\n",
    "infixl 4 ~>\n",
    "(~>) :: ForwardLayer a -> ForwardLayer a -> ForwardLayer a\n",
    "a ~> (JoinedForwardLayer x y) = (a ~> x) ~> y\n",
    "a ~> b = JoinedForwardLayer a b\n",
    "\n",
    "data OutputLayer a = SoftmaxWithCrossForward\n",
    "  deriving (Show)\n",
    "\n",
    "data ForwardNN a = ForwardNN (ForwardLayer a) (OutputLayer a)\n",
    "  deriving (Show)\n",
    "\n",
    "data BackwardLayer a = \n",
    "    AffineBackward (Weight a) (Bias a) (SignalX a)\n",
    "  | SigmoidBackward (SignalY a)\n",
    "  | ReLUBackward (SignalX a)\n",
    "  | JoinedBackwardLayer (BackwardLayer a) (BackwardLayer a)\n",
    "  deriving (Show)\n",
    "\n",
    "infixr 4 <~\n",
    "(<~) :: BackwardLayer a -> BackwardLayer a -> BackwardLayer a\n",
    "(JoinedBackwardLayer x y) <~ b = x <~ (y <~ b)\n",
    "a <~ b = JoinedBackwardLayer a b\n",
    "\n",
    "data BackputLayer a = SoftmaxWithCrossBackward (TeacherBatch a) (SignalY a)\n",
    "\n",
    "data BackwardNN a = BackwardNN (BackwardLayer a) (BackputLayer a)\n",
    "\n",
    "type NElement a = (Ord a, Floating a, Numeric a, Num (Vector a), Show a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward :: NElement a => ForwardLayer a -> SignalX a -> (BackwardLayer a, SignalY a)\n",
    "forward (AffineForward w b) x = (AffineBackward w b x, affinem w b x)\n",
    "forward SigmoidForward x = let y = sigmoidm x in (SigmoidBackward y, y)\n",
    "forward ReLUForward x = (ReLUBackward x, relum x)\n",
    "forward (JoinedForwardLayer a b) x0 = (a' <~ b', x2)\n",
    "    where\n",
    "        (a', x1) = forward a x0\n",
    "        (b', x2) = forward b x1\n",
    "\n",
    "backward :: NElement a => a -> BackwardLayer a -> Diff a -> (ForwardLayer a, Diff a)\n",
    "backward rate (AffineBackward w b x) d =  (AffineForward (w - scale rate w') (b - scale rate b'), x')\n",
    "    where (x', w', b') = affinemBackward w x d\n",
    "backward _ (SigmoidBackward y) d = (SigmoidForward, sigmoidBackward y d)\n",
    "backward _ (ReLUBackward x) d = (ReLUForward, relumBackward x d)\n",
    "backward r (JoinedBackwardLayer a b) d0 = (a' ~> b', d2)\n",
    "    where\n",
    "        (b', d1) = backward r b d0\n",
    "        (a', d2) = backward r a d1\n",
    "\n",
    "output :: NElement a => OutputLayer a -> TeacherBatch a -> SignalY a -> (BackputLayer a, a)\n",
    "output SoftmaxWithCrossForward t y = (SoftmaxWithCrossBackward t y', loss)\n",
    "    where\n",
    "        (y', loss) = softmaxWithCross t y\n",
    "\n",
    "backput :: NElement a => BackputLayer a -> (OutputLayer a, Diff a)\n",
    "backput (SoftmaxWithCrossBackward t y) = (SoftmaxWithCrossForward, softmaxWithCrossBackward t y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnForward :: NElement a => ForwardNN a -> TrainBatch a -> (BackwardNN a, a)\n",
    "learnForward (ForwardNN layers loss) (TrainBatch (t, x)) = result `seq` (BackwardNN layers' loss', result)\n",
    "    where\n",
    "        (layers', y) = forward layers x\n",
    "        (loss', result) = output loss t y\n",
    "\n",
    "learnBackward :: NElement a => a -> BackwardNN a -> ForwardNN a\n",
    "learnBackward  rate (BackwardNN layers loss) = ForwardNN layers' loss'\n",
    "    where\n",
    "        (loss', d) = backput loss\n",
    "        (layers', _) = backward rate layers d\n",
    "\n",
    "learn :: NElement a => a -> ForwardNN a -> TrainBatch a -> (ForwardNN a, a)\n",
    "learn rate a = first (learnBackward rate) . learnForward a\n",
    "\n",
    "learnAll :: NElement a => a -> ForwardNN a -> [TrainBatch a] -> (ForwardNN a, [a])\n",
    "learnAll rate origin = foldr f (origin, [])\n",
    "    where f batch (a, results) = trace (\"Learning iterate: \" ++ show (length results + 1)) $ a `seq` second (:results) $ learn rate a batch\n",
    "\n",
    "predict :: NElement a => ForwardLayer a -> Vector a -> Int\n",
    "-- predict layers = maxIndex . flatten . snd . (forward layers) . asRow\n",
    "predict layers sample = maxIndex $ trace (\"Evaluated result: \" ++ show result) result\n",
    "    where\n",
    "        m1 = asRow sample\n",
    "        (_, m2) = forward layers m1\n",
    "        result = flatten m2\n",
    "\n",
    "evaluate :: NElement a => ForwardLayer a -> [(Int, Vector a)] -> Double\n",
    "evaluate layers samples = fromIntegral nOk / fromIntegral (length samples)\n",
    "    where\n",
    "        nOk = length $ filter (uncurry (==)) results\n",
    "        results = map (second $ predict layers) samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "newtype MnistLabels = MnistLabels [Word8] deriving (Show)\n",
    "newtype MnistImages = MnistImages [Matrix Z] deriving (Show)\n",
    "newtype MnistData = MnistData [(Word8, Matrix Z)] deriving (Show)\n",
    "\n",
    "zipMnist :: MnistLabels -> MnistImages -> MnistData\n",
    "zipMnist (MnistLabels labels) (MnistImages images) = list `deepseq` MnistData list\n",
    "    where list = labels `zip` images\n",
    "\n",
    "markMinistLabels = 2049\n",
    "markMnistImages = 2051\n",
    "\n",
    "putAsWord32 :: Integral a => a -> Put\n",
    "putAsWord32 a = B.put (fromIntegral a :: Word32)\n",
    "\n",
    "getAsIntegral :: Integral a => Get a\n",
    "getAsIntegral = fromIntegral <$> (B.get :: Get Word32)\n",
    "\n",
    "instance B.Binary MnistLabels where\n",
    "    put (MnistLabels labels) = do\n",
    "        B.put markMinistLabels\n",
    "        putAsWord32 $ length labels\n",
    "        B.put $ BS.pack labels\n",
    "\n",
    "    get = do\n",
    "        mark <- getAsIntegral\n",
    "        guard $ mark == markMinistLabels\n",
    "        size <- getAsIntegral\n",
    "        let total = trace (\"Reading labels: \" ++ show size) size\n",
    "        labels <- BGI.readN total BSS.unpack\n",
    "        return $ MnistLabels $ labels\n",
    "\n",
    "instance B.Binary MnistImages where\n",
    "    put (MnistImages (images @ (hm:_))) = do\n",
    "        putAsWord32 markMnistImages\n",
    "        putAsWord32 $ length images\n",
    "        putAsWord32 $ rows hm\n",
    "        putAsWord32 $ cols hm\n",
    "        B.put $ (BS.pack . map fromIntegral . concat . concat . map toLists) images\n",
    "\n",
    "    get = do\n",
    "        mark <- getAsIntegral\n",
    "        guard $ mark == markMnistImages\n",
    "        size <- getAsIntegral\n",
    "        nRows <- getAsIntegral\n",
    "        nCols <- getAsIntegral\n",
    "        let total = trace (\"Reading images: \" ++ show size ++ \" of \" ++ show nRows ++ \"x\" ++ show nCols) size * nRows * nCols\n",
    "        zs <- map fromIntegral <$> BGI.readN total BSS.unpack\n",
    "        let images = map (nRows >< nCols) $ chunksOf (nRows * nCols) zs\n",
    "        return $ MnistImages images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "timestamp :: String -> IO ()\n",
    "timestamp msg = msg `deepseq` do\n",
    "    t <- getZonedTime\n",
    "    let fm = \"[%Y/%m/%d %H:%M:%S] \" ++ msg\n",
    "    print $ formatTime defaultTimeLocale fm t\n",
    "\n",
    "download :: Manager -> String -> IO BS.ByteString\n",
    "download manager url = do\n",
    "   request <- parseRequest url\n",
    "   timestamp $ \"Downloading \" ++ url\n",
    "   response <- httpLbs request manager\n",
    "   return $ responseBody response\n",
    "\n",
    "saveMnist :: FilePath -> String -> [String] -> Manager -> IO [FilePath]\n",
    "saveMnist rootDir urlBase filenames manager = do\n",
    "    isDir <- doesDirectoryExist rootDir\n",
    "    _ <- if isDir then return () else createDirectory rootDir\n",
    "    mapM saveFile filenames\n",
    "    where\n",
    "        saveFile filename = do\n",
    "            let url = urlBase ++ filename\n",
    "            let file = rootDir </> filename\n",
    "            timestamp $ \"Checking file \" ++ file\n",
    "            e <- doesFileExist file\n",
    "            if e then return () else download manager url >>= (BS.writeFile file)\n",
    "            return file\n",
    "\n",
    "readMnist :: FilePath -> IO (Either MnistLabels MnistImages)\n",
    "readMnist file = do\n",
    "    timestamp $ \"decoding \" ++ file ++ \" ...\"\n",
    "    bs <- GZip.decompress <$> BS.readFile file\n",
    "    let a = left (const $ B.decode bs) $ right (\\(_, _, a) -> a) $ B.decodeOrFail bs\n",
    "    timestamp $ \"decoded \" ++ file\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffleList :: [a] -> IO [a]\n",
    "shuffleList [a] = return [a]\n",
    "shuffleList xs =  withSystemRandom . asGenST $ \\g -> do\n",
    "    v <- VG.unsafeThaw $ VG.fromList xs\n",
    "    repeatSwap v g n\n",
    "    (v' :: V.Vector a) <- VG.unsafeFreeze v\n",
    "    return $ VG.toList v'\n",
    "    where\n",
    "        n = length xs - 1\n",
    "        repeatSwap v g i = if (i < 0) then return () else do\n",
    "            j <- uniformR (0, n) g\n",
    "            VM.swap v j i\n",
    "            repeatSwap v g (i - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do\n",
    "    let src = [1..60000]\n",
    "    let len = length src\n",
    "    timestamp (\"Start shuffling...\" ++ show len)\n",
    "    start <- getCurrentTime\n",
    "    list <- shuffleList src\n",
    "    end <- list `seq` getCurrentTime\n",
    "    let dur = end `diffUTCTime` start\n",
    "    timestamp $ \"Finish shuffle list: length=\" ++ show len ++ \" took \" ++ show dur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "normalAffine :: Int -> Int -> IO (ForwardLayer R)\n",
    "normalAffine nIn nOut = do\n",
    "        weights <- trace (\"Random Affine: \" ++ show nIn ++ \"x\" ++ show nOut) rand nIn nOut\n",
    "        bias <- flatten <$> rand nOut 1\n",
    "        return $ AffineForward weights bias\n",
    "\n",
    "initNN :: [Int] -> IO (ForwardNN R)\n",
    "initNN ns = do\n",
    "    (lastAffine:affines) <- mapM (uncurry normalAffine) $ spans [] ns\n",
    "    let layers = foldl' (\\b a -> a ~> ReLUForward ~> b) lastAffine affines\n",
    "    return $ ForwardNN layers SoftmaxWithCrossForward\n",
    "        where\n",
    "            spans rs (a:b:[]) = (a, b):rs\n",
    "            spans rs (a:b:xs) = spans ((a, b):rs) (b:xs)\n",
    "\n",
    "convertTrains :: Int -> MnistData -> [TrainBatch R]\n",
    "convertTrains batchSize (MnistData src) = map (mkTrainer . unzip) $ chunksOf batchSize $ vectors\n",
    "    where\n",
    "        vectors = map (first (hotone 10) . second flatten) src\n",
    "        mkTrainer (a, b) = TrainBatch (fromRows a, (fromZ $ fromRows b) / 255)\n",
    "\n",
    "hotone :: (Integral v, NElement a) => Int -> v -> (Vector a)\n",
    "hotone n v = fromList $ map fromIntegral list\n",
    "    where\n",
    "        list = replicate i 0 ++ 1 : replicate (n - i - 1) 0\n",
    "        i = fromIntegral v\n",
    "\n",
    "convertTests :: MnistData -> [(Int, Vector R)]\n",
    "convertTests (MnistData src) = map (first fromIntegral . second ((/255) . flatten . fromZ)) src\n",
    "\n",
    "shuffleMnist :: MnistData -> IO MnistData\n",
    "shuffleMnist (MnistData list) = MnistData <$> shuffleList list\n",
    "\n",
    "trainingSimple :: Double -> ForwardNN R -> MnistData -> MnistData -> IO Double\n",
    "trainingSimple rate nn trainData testData = do\n",
    "    trainBatches <- convertTrains 100 <$> shuffleMnist trainData\n",
    "    let batches = concat $ replicate 10 trainBatches\n",
    "    let (ForwardNN layers _, _) = learnAll rate nn batches\n",
    "    return $ evaluate layers $ convertTests testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "newManager defaultManagerSettings >>= saveMnist \"mnist\" \"http://yann.lecun.com/exdb/mnist/\" [\n",
    "    \"train-images-idx3-ubyte.gz\"\n",
    "  , \"train-labels-idx1-ubyte.gz\"\n",
    "  , \"t10k-images-idx3-ubyte.gz\"\n",
    "  , \"t10k-labels-idx1-ubyte.gz\"\n",
    "  ] >>= mapM readMnist >>= \\ms -> do\n",
    "      let [trainers, tests] = map (\\[Right a, Left b] -> b `zipMnist` a) $ chunksOf 2 ms\n",
    "      origin <- initNN [28*28, 50, 100, 10]\n",
    "      result <- trainingSimple 0.1 origin trainers tests\n",
    "      timestamp $ \"result=\" ++ show (result * 100) ++ \"%\"\n",
    "      return result"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Haskell",
   "language": "haskell",
   "name": "haskell"
  },
  "language_info": {
   "codemirror_mode": "ihaskell",
   "file_extension": ".hs",
   "name": "haskell",
   "pygments_lexer": "Haskell",
   "version": "8.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
