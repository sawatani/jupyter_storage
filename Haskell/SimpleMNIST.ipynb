{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{-# LANGUAGE FunctionalDependencies #-}\n",
    "{-# LANGUAGE MonomorphismRestriction #-}\n",
    "{-# LANGUAGE FlexibleContexts #-}\n",
    "{-# LANGUAGE ConstraintKinds #-}\n",
    "\n",
    "import Control.Arrow\n",
    "import Control.Monad.State\n",
    "import Numeric.LinearAlgebra\n",
    "import Numeric.LinearAlgebra.Data\n",
    "import Numeric\n",
    "import Prelude hiding ((<>))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "Sigmoid \\nonumber \\\\\n",
    "  h(x) &=& \\frac\n",
    "  {1}\n",
    "  {1 + \\exp(-x)} \\\\\n",
    "\\nonumber \\\\\n",
    "ReLU \\nonumber \\\\\n",
    "  h(x) &=& \\begin{cases}\n",
    "  x & (x > 0) \\\\\n",
    "  0 & (x \\le 0)\n",
    "  \\end{cases}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid :: (Floating a) => a -> a\n",
    "sigmoid a = fromIntegral 1 / (fromIntegral 1 + exp (-a))\n",
    "\n",
    "sigmoidm :: (Floating a, Container Matrix a) => Matrix a -> Matrix a\n",
    "sigmoidm = cmap sigmoid\n",
    "\n",
    "sigmoidBackward :: (Floating a, Num (Vector a), Container Matrix a) => Matrix a -> Matrix a -> Matrix a\n",
    "sigmoidBackward y d = d * (cmap (\\a -> fromIntegral 1 - a) y) * y\n",
    "\n",
    "relu :: (Ord a, Num a) => a -> a\n",
    "relu = max $ fromIntegral 0\n",
    "\n",
    "relum :: (Ord a, Num a, Container Matrix a) => Matrix a -> Matrix a\n",
    "relum = cmap relu\n",
    "\n",
    "relumBackward :: (Ord a, Num a, Num (Matrix a), Container Vector a, Container Matrix a) => Matrix a -> Matrix a -> Matrix a\n",
    "relumBackward x d = d * mask\n",
    "    where mask = cmap (\\a -> fromIntegral $ if (0 < a) then 1 else 0) x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "Cross Entropy \\nonumber \\\\\n",
    "  L &=& -\\displaystyle \\sum_{i=1}^{n}t_i \\log (y_i + d) \\\\\n",
    "  t &:& 教師ラベル \\nonumber \\\\\n",
    "  d &:& 微小値 \\nonumber \\\\\n",
    " \\nonumber \\\\\n",
    "Softmax \\nonumber \\\\\n",
    "  y_k &=& \\frac\n",
    "  {\\exp(a_k - \\hat{a})}\n",
    "  {\\displaystyle \\sum_{i=1}^{n}\\exp(a_i - \\hat{a})} \\\\\n",
    "  \\hat{a} &=& \\max \\{ a_{1...n} \\} \\nonumber\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax :: (Floating a, Container Vector a) => Vector a -> Vector a\n",
    "softmax v = cmap (/s) v'\n",
    "    where\n",
    "    m = maxElement v\n",
    "    v' = cmap (\\a -> exp (a - m)) v\n",
    "    s = sumElements v'\n",
    "\n",
    "softmaxm :: (Floating a, Container Vector a) => Matrix a -> Matrix a\n",
    "softmaxm m = fromRows $ map softmax $ toRows m\n",
    "\n",
    "crossEntropy :: (Floating a, Num (Vector a), Container Vector a) => Vector a -> Vector a -> a\n",
    "crossEntropy t y = -(sumElements $ cmap log (y + d) * t)\n",
    "    where d = 1e-10\n",
    "\n",
    "crossEntropym :: (Floating a, Num (Vector a), Container Vector a) => Matrix a -> Matrix a -> a\n",
    "crossEntropym t m = sum vs / batchSize\n",
    "    where \n",
    "    vs = uncurry crossEntropy `map` (toRows t `zip` toRows m)\n",
    "    batchSize = fromIntegral $ rows t\n",
    "\n",
    "softmaxWithCross :: (Floating a, Num (Vector a), Container Vector a) => Matrix a -> Matrix a -> a\n",
    "softmaxWithCross t = crossEntropym t . softmaxm\n",
    "\n",
    "softmaxWithCrossBackward :: (Floating a, Num (Vector a), Container Vector a) => Matrix a -> Matrix a -> Matrix a\n",
    "softmaxWithCrossBackward t y = (y - t) / batchSize\n",
    "    where batchSize = fromIntegral $ rows t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affinem :: (Floating a, Numeric a, Num (Vector a)) => Matrix a -> Vector a -> Matrix a -> Matrix a\n",
    "affinem w b x = x <> w + b'\n",
    "    where b' = fromColumns $ replicate (rows x) b\n",
    "\n",
    "affinemBackward :: (Floating a, Numeric a) => Matrix a -> Matrix a -> Matrix a -> (Matrix a, Matrix a, Vector a)\n",
    "affinemBackward w x d = (dx, dw, db)\n",
    "    where\n",
    "    dx = d <> tr w\n",
    "    dw = tr x <> d\n",
    "    db = fromList $ sumElements `map` toColumns d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type Weight a = Matrix a\n",
    "type Bias a = Vector a\n",
    "type Signal a = Matrix a\n",
    "type Teachers a = Matrix a\n",
    "type Teachers a = Matrix a\n",
    "newtype LearnData a = LearnData (Matrix a, Matrix a)\n",
    "\n",
    "data ForwardLayer a =\n",
    "    AffineForward (Matrix a) (Vector a)\n",
    "  | SigmoidForward\n",
    "  | ReLUForward\n",
    "  | JoinedForwardLayer (ForwardLayer a) (ForwardLayer a)\n",
    "  deriving (Show)\n",
    "\n",
    "infixl 4 ~>\n",
    "(~>) :: ForwardLayer a -> ForwardLayer a -> ForwardLayer a\n",
    "a ~> (JoinedForwardLayer x y) = (a ~> x) ~> y\n",
    "a ~> b = JoinedForwardLayer a b\n",
    "\n",
    "data OutputLayer a = SoftmaxWithCrossForward\n",
    "  deriving (Show)\n",
    "\n",
    "data ForwardNN a = ForwardNN (ForwardLayer a) (OutputLayer a)\n",
    "  deriving (Show)\n",
    "\n",
    "data BackwardLayer a = \n",
    "    AffineBackward (Matrix a) (Vector a) (Matrix a)\n",
    "  | SigmoidBackward (Matrix a)\n",
    "  | ReLUBackward (Matrix a)\n",
    "  | JoinedBackwardLayer (BackwardLayer a) (BackwardLayer a)\n",
    "  deriving (Show)\n",
    "\n",
    "infixr 4 <~\n",
    "(<~) :: BackwardLayer a -> BackwardLayer a -> BackwardLayer a\n",
    "(JoinedBackwardLayer x y) <~ b = x <~ (y <~ b)\n",
    "a <~ b = JoinedBackwardLayer a b\n",
    "\n",
    "data BackputLayer a = SoftmaxWithCrossBackward (Matrix a) (Matrix a)\n",
    "\n",
    "data BackwardNN a = BackwardNN (BackwardLayer a) (BackputLayer a)\n",
    "\n",
    "type NElement a = (Ord a, Floating a, Numeric a, Num (Vector a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward :: NElement a => ForwardLayer a -> Matrix a -> (BackwardLayer a, Matrix a)\n",
    "forward (AffineForward w b) x = (AffineBackward w b x, affinem w b x)\n",
    "forward SigmoidForward x = let y = sigmoidm x in (SigmoidBackward y, y)\n",
    "forward ReLUForward x = (ReLUBackward x, relum x)\n",
    "forward (JoinedForwardLayer a b) x0 = (a' <~ b', x2)\n",
    "    where\n",
    "    (a', x1) = forward a x0\n",
    "    (b', x2) = forward b x1\n",
    "\n",
    "backward :: NElement a => BackwardLayer a -> Matrix a -> (ForwardLayer a, Matrix a)\n",
    "backward (AffineBackward w b x) d =  (AffineForward (w + w') (b + b'), x')\n",
    "    where (x', w', b') = affinemBackward w x d\n",
    "backward (SigmoidBackward y) d = (SigmoidForward, sigmoidBackward y d)\n",
    "backward (ReLUBackward x) d = (ReLUForward, relumBackward x d)\n",
    "backward (JoinedBackwardLayer a b) d0 = (a' ~> b', d2)\n",
    "    where\n",
    "    (b', d1) = backward b d0\n",
    "    (a', d2) = backward a d1\n",
    "\n",
    "output :: NElement a => OutputLayer a -> Matrix a -> Matrix a -> (BackputLayer a, a)\n",
    "output SoftmaxWithCrossForward t y = (SoftmaxWithCrossBackward t y, softmaxWithCross t y)\n",
    "\n",
    "backput :: NElement a => BackputLayer a -> (OutputLayer a, Matrix a)\n",
    "backput (SoftmaxWithCrossBackward t y) = (SoftmaxWithCrossForward, softmaxWithCrossBackward t y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnForward :: NElement a => ForwardNN a -> LearnData a -> (BackwardNN a, a)\n",
    "learnForward (ForwardNN layers loss) (LearnData (t, x)) = (BackwardNN layers' loss', result)\n",
    "    where\n",
    "    (layers', y) = forward layers x\n",
    "    (loss', result) = output loss t y\n",
    "\n",
    "learnBackward :: NElement a => BackwardNN a -> ForwardNN a\n",
    "learnBackward  (BackwardNN layers loss) = ForwardNN layers' loss'\n",
    "    where\n",
    "    (loss', d) = backput loss\n",
    "    (layers', _) = backward layers d\n",
    "\n",
    "learn :: NElement a => ForwardNN a -> LearnData a -> (ForwardNN a, a)\n",
    "learn a = first learnBackward . learnForward a\n",
    "\n",
    "learnAll :: NElement a => ForwardNN a -> [LearnData a] -> (ForwardNN a, [a])\n",
    "learnAll origin = foldr f (origin, [])\n",
    "    where f p (a, results) = second (: results) $ learn a p\n",
    "\n",
    "predict :: NElement a => ForwardLayer a -> Vector a -> Int\n",
    "predict layers = maxIndex . flatten . snd . (forward layers) . asRow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = SigmoidForward\n",
    "a ~> (a ~> a) ~> a\n",
    "b = SigmoidBackward (matrix 1 [0])\n",
    "b <~ (b <~ b) <~ b"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Haskell",
   "language": "haskell",
   "name": "haskell"
  },
  "language_info": {
   "codemirror_mode": "ihaskell",
   "file_extension": ".hs",
   "name": "haskell",
   "pygments_lexer": "Haskell",
   "version": "8.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
