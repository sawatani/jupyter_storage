{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{-# LANGUAGE FunctionalDependencies #-}\n",
    "{-# LANGUAGE MonomorphismRestriction #-}\n",
    "{-# LANGUAGE FlexibleContexts #-}\n",
    "{-# LANGUAGE ConstraintKinds #-}\n",
    "{-# LANGUAGE TemplateHaskell #-}\n",
    "{-# LANGUAGE OverloadedStrings #-}\n",
    "\n",
    "import Control.Arrow\n",
    "import Control.Monad.State\n",
    "import Control.Monad.Trans.Class\n",
    "import Numeric.LinearAlgebra\n",
    "import Numeric.LinearAlgebra.Data\n",
    "import Data.List.Split\n",
    "import Numeric\n",
    "import Prelude hiding ((<>))\n",
    "import Network.HTTP.Client\n",
    "import Network.HTTP.Types.Status (statusCode)\n",
    "import qualified Codec.Compression.GZip as GZip\n",
    "import System.Directory\n",
    "import System.FilePath.Posix\n",
    "import qualified Data.ByteString.Lazy  as BS\n",
    "import qualified Data.ByteString as BSS\n",
    "import Data.Binary as B\n",
    "import Data.Time.Format\n",
    "import Data.Time.LocalTime\n",
    "import Debug.Trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "Sigmoid \\nonumber \\\\\n",
    "  h(x) &=& \\frac\n",
    "  {1}\n",
    "  {1 + \\exp(-x)} \\\\\n",
    "\\nonumber \\\\\n",
    "ReLU \\nonumber \\\\\n",
    "  h(x) &=& \\begin{cases}\n",
    "  x & (x > 0) \\\\\n",
    "  0 & (x \\le 0)\n",
    "  \\end{cases}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid :: (Floating a) => a -> a\n",
    "sigmoid a = fromIntegral 1 / (fromIntegral 1 + exp (-a))\n",
    "\n",
    "sigmoidm :: (Floating a, Container Matrix a) => Matrix a -> Matrix a\n",
    "sigmoidm = cmap sigmoid\n",
    "\n",
    "sigmoidBackward :: (Floating a, Num (Vector a), Container Matrix a) => Matrix a -> Matrix a -> Matrix a\n",
    "sigmoidBackward y d = d * (cmap (\\a -> fromIntegral 1 - a) y) * y\n",
    "\n",
    "relu :: (Ord a, Num a) => a -> a\n",
    "relu = max $ fromIntegral 0\n",
    "\n",
    "relum :: (Ord a, Num a, Container Matrix a) => Matrix a -> Matrix a\n",
    "relum = cmap relu\n",
    "\n",
    "relumBackward :: (Ord a, Num a, Num (Matrix a), Container Vector a, Container Matrix a) => Matrix a -> Matrix a -> Matrix a\n",
    "relumBackward x d = d * mask\n",
    "    where mask = cmap (\\a -> fromIntegral $ if (0 < a) then 1 else 0) x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "Cross Entropy \\nonumber \\\\\n",
    "  L &=& -\\displaystyle \\sum_{i=1}^{n}t_i \\log (y_i + d) \\\\\n",
    "  t &:& 教師ラベル \\nonumber \\\\\n",
    "  d &:& 微小値 \\nonumber \\\\\n",
    " \\nonumber \\\\\n",
    "Softmax \\nonumber \\\\\n",
    "  y_k &=& \\frac\n",
    "  {\\exp(a_k - \\hat{a})}\n",
    "  {\\displaystyle \\sum_{i=1}^{n}\\exp(a_i - \\hat{a})} \\\\\n",
    "  \\hat{a} &=& \\max \\{ a_{1...n} \\} \\nonumber\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax :: (Floating a, Container Vector a) => Vector a -> Vector a\n",
    "softmax v = cmap (/s) v'\n",
    "    where\n",
    "    m = maxElement v\n",
    "    v' = cmap (\\a -> exp (a - m)) v\n",
    "    s = sumElements v'\n",
    "\n",
    "softmaxm :: (Floating a, Container Vector a) => Matrix a -> Matrix a\n",
    "softmaxm m = fromRows $ map softmax $ toRows m\n",
    "\n",
    "crossEntropy :: (Floating a, Num (Vector a), Container Vector a) => Vector a -> Vector a -> a\n",
    "crossEntropy t y = -(sumElements $ cmap log (y + d) * t)\n",
    "    where d = 1e-10\n",
    "\n",
    "crossEntropym :: (Floating a, Num (Vector a), Container Vector a) => Matrix a -> Matrix a -> a\n",
    "crossEntropym t m = sum vs / batchSize\n",
    "    where \n",
    "    vs = uncurry crossEntropy `map` (toRows t `zip` toRows m)\n",
    "    batchSize = fromIntegral $ rows t\n",
    "\n",
    "softmaxWithCross :: (Floating a, Num (Vector a), Container Vector a) => Matrix a -> Matrix a -> a\n",
    "softmaxWithCross t = crossEntropym t . softmaxm\n",
    "\n",
    "softmaxWithCrossBackward :: (Floating a, Num (Vector a), Container Vector a) => Matrix a -> Matrix a -> Matrix a\n",
    "softmaxWithCrossBackward t y = (y - t) / batchSize\n",
    "    where batchSize = fromIntegral $ rows t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affinem :: (Floating a, Numeric a, Num (Vector a)) => Matrix a -> Vector a -> Matrix a -> Matrix a\n",
    "affinem w b x = x <> w + b'\n",
    "    where b' = fromColumns $ replicate (rows x) b\n",
    "\n",
    "affinemBackward :: (Floating a, Numeric a) => Matrix a -> Matrix a -> Matrix a -> (Matrix a, Matrix a, Vector a)\n",
    "affinemBackward w x d = (dx, dw, db)\n",
    "    where\n",
    "    dx = d <> tr w\n",
    "    dw = tr x <> d\n",
    "    db = fromList $ sumElements `map` toColumns d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type Weight a = Matrix a\n",
    "type Bias a = Vector a\n",
    "type SignalX a = Matrix a\n",
    "type SignalY a = Matrix a\n",
    "type Diff a = Matrix a\n",
    "type TeacherBatch a = Matrix a\n",
    "type InputBatch a = Matrix a\n",
    "newtype TrainBatch a = TrainBatch (TeacherBatch a, InputBatch a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data ForwardLayer a =\n",
    "    AffineForward (Weight a) (Bias a)\n",
    "  | SigmoidForward\n",
    "  | ReLUForward\n",
    "  | JoinedForwardLayer (ForwardLayer a) (ForwardLayer a)\n",
    "  deriving (Show)\n",
    "\n",
    "infixl 4 ~>\n",
    "(~>) :: ForwardLayer a -> ForwardLayer a -> ForwardLayer a\n",
    "a ~> (JoinedForwardLayer x y) = (a ~> x) ~> y\n",
    "a ~> b = JoinedForwardLayer a b\n",
    "\n",
    "data OutputLayer a = SoftmaxWithCrossForward\n",
    "  deriving (Show)\n",
    "\n",
    "data ForwardNN a = ForwardNN (ForwardLayer a) (OutputLayer a)\n",
    "  deriving (Show)\n",
    "\n",
    "data BackwardLayer a = \n",
    "    AffineBackward (Weight a) (Bias a) (SignalX a)\n",
    "  | SigmoidBackward (SignalY a)\n",
    "  | ReLUBackward (SignalX a)\n",
    "  | JoinedBackwardLayer (BackwardLayer a) (BackwardLayer a)\n",
    "  deriving (Show)\n",
    "\n",
    "infixr 4 <~\n",
    "(<~) :: BackwardLayer a -> BackwardLayer a -> BackwardLayer a\n",
    "(JoinedBackwardLayer x y) <~ b = x <~ (y <~ b)\n",
    "a <~ b = JoinedBackwardLayer a b\n",
    "\n",
    "data BackputLayer a = SoftmaxWithCrossBackward (TeacherBatch a) (SignalY a)\n",
    "\n",
    "data BackwardNN a = BackwardNN (BackwardLayer a) (BackputLayer a)\n",
    "\n",
    "type NElement a = (Ord a, Floating a, Numeric a, Num (Vector a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward :: NElement a => ForwardLayer a -> SignalX a -> (BackwardLayer a, SignalY a)\n",
    "forward (AffineForward w b) x = (AffineBackward w b x, affinem w b x)\n",
    "forward SigmoidForward x = let y = sigmoidm x in (SigmoidBackward y, y)\n",
    "forward ReLUForward x = (ReLUBackward x, relum x)\n",
    "forward (JoinedForwardLayer a b) x0 = (a' <~ b', x2)\n",
    "    where\n",
    "    (a', x1) = forward a x0\n",
    "    (b', x2) = forward b x1\n",
    "\n",
    "backward :: NElement a => BackwardLayer a -> Diff a -> (ForwardLayer a, Diff a)\n",
    "backward (AffineBackward w b x) d =  (AffineForward (w + w') (b + b'), x')\n",
    "    where (x', w', b') = affinemBackward w x d\n",
    "backward (SigmoidBackward y) d = (SigmoidForward, sigmoidBackward y d)\n",
    "backward (ReLUBackward x) d = (ReLUForward, relumBackward x d)\n",
    "backward (JoinedBackwardLayer a b) d0 = (a' ~> b', d2)\n",
    "    where\n",
    "    (b', d1) = backward b d0\n",
    "    (a', d2) = backward a d1\n",
    "\n",
    "output :: NElement a => OutputLayer a -> TeacherBatch a -> SignalY a -> (BackputLayer a, a)\n",
    "output SoftmaxWithCrossForward t y = (SoftmaxWithCrossBackward t y, softmaxWithCross t y)\n",
    "\n",
    "backput :: NElement a => BackputLayer a -> (OutputLayer a, Diff a)\n",
    "backput (SoftmaxWithCrossBackward t y) = (SoftmaxWithCrossForward, softmaxWithCrossBackward t y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnForward :: NElement a => ForwardNN a -> TrainBatch a -> (BackwardNN a, a)\n",
    "learnForward (ForwardNN layers loss) (TrainBatch (t, x)) = (BackwardNN layers' loss', result)\n",
    "    where\n",
    "    (layers', y) = forward layers x\n",
    "    (loss', result) = output loss t y\n",
    "\n",
    "learnBackward :: NElement a => BackwardNN a -> ForwardNN a\n",
    "learnBackward  (BackwardNN layers loss) = ForwardNN layers' loss'\n",
    "    where\n",
    "    (loss', d) = backput loss\n",
    "    (layers', _) = backward layers d\n",
    "\n",
    "learn :: NElement a => ForwardNN a -> TrainBatch a -> (ForwardNN a, a)\n",
    "learn a = first learnBackward . learnForward a\n",
    "\n",
    "learnAll :: NElement a => ForwardNN a -> [TrainBatch a] -> (ForwardNN a, [a])\n",
    "learnAll origin = foldr f (origin, [])\n",
    "    where f p (a, results) = second (: results) $ learn a p\n",
    "\n",
    "predict :: NElement a => ForwardLayer a -> Vector a -> Int\n",
    "predict layers = maxIndex . flatten . snd . (forward layers) . asRow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtype MnistLabels = MnistLabels [Word8] deriving (Show)\n",
    "newtype MnistImages = MnistImages [Matrix Z] deriving (Show)\n",
    "newtype MnistData = MnistData [(Word8, Matrix Z)] deriving (Show)\n",
    "\n",
    "zipMnist :: MnistLabels -> MnistImages -> MnistData\n",
    "zipMnist (MnistLabels labels) (MnistImages images) = MnistData $ labels `zip` images\n",
    "\n",
    "markMinistLabels = 2049\n",
    "markMnistImages = 2051\n",
    "\n",
    "putWord32 :: Integral a => a -> Put\n",
    "putWord32 a = B.put (fromIntegral a :: Word32)\n",
    "\n",
    "getWord32 :: Get Int\n",
    "getWord32 = fromIntegral <$> (B.get :: Get Word32)\n",
    "\n",
    "putOnes :: Binary a => [a] -> Put\n",
    "putOnes [] = return ()\n",
    "putOnes (x:xs) = B.put x >> putOnes xs\n",
    "\n",
    "getOnes :: Binary a => Int -> Get [a]\n",
    "getOnes total = doGet total []\n",
    "    where\n",
    "    doGet 0 xs = return $! reverse xs\n",
    "    doGet n xs = do\n",
    "        x <- B.get\n",
    "        x `seq` doGet (n - 1) (x:xs)\n",
    "\n",
    "instance B.Binary MnistLabels where\n",
    "    put (MnistLabels labels) = do\n",
    "        B.put markMinistLabels\n",
    "        putWord32 $ length labels\n",
    "        putOnes labels\n",
    "\n",
    "    get = do\n",
    "        mark <- getWord32\n",
    "        guard $ mark == markMinistLabels\n",
    "        size <- getWord32\n",
    "        labels <- getOnes $ trace (\"Reading labels: \" ++ show size) size\n",
    "        return $ MnistLabels labels\n",
    "\n",
    "instance B.Binary MnistImages where\n",
    "    put (MnistImages (images @ (hm:_))) = do\n",
    "        putWord32 markMnistImages\n",
    "        putWord32 $ length images\n",
    "        putWord32 $ rows hm\n",
    "        putWord32 $ cols hm\n",
    "        putOnes xs\n",
    "        where\n",
    "        zs = concat $ concat $ map toLists images\n",
    "        xs = map fromIntegral zs :: [Word32]\n",
    "\n",
    "    get = do\n",
    "        mark <- getWord32\n",
    "        guard $ mark == markMnistImages\n",
    "        size <- getWord32\n",
    "        nRows <- getWord32\n",
    "        nCols <- getWord32\n",
    "        let total = trace (\"Reading images: \" ++ show size ++ \" of \" ++ show nRows ++ \"x\" ++ show nCols) size * nRows * nCols\n",
    "        xs' <- getOnes total\n",
    "        let xs = trace (\"Loaded elements: \" ++ show size) xs'\n",
    "        let zs' = map fromIntegral (xs :: [Word8])\n",
    "        let zs = trace (\"Converted elements: \" ++ show size) zs'\n",
    "        let images' = map (nRows >< nCols) $ chunksOf (nRows * nCols) zs\n",
    "        let images = trace \"Built images\" images'\n",
    "        return $ MnistImages images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp msg = do\n",
    "    t <- getZonedTime\n",
    "    let stamp = formatTime defaultTimeLocale \"[%Y/%m/%d %H:%M:%S] \" t\n",
    "    print $ stamp ++ msg\n",
    "\n",
    "download :: Manager -> String -> IO BS.ByteString\n",
    "download manager url = do\n",
    "   request <- parseRequest url\n",
    "   timestamp $ \"Downloading \" ++ url\n",
    "   response <- httpLbs request manager\n",
    "   let bs = GZip.decompress $ responseBody response\n",
    "   return bs\n",
    "\n",
    "saveMnist :: FilePath -> String -> [(String, String)] -> Manager -> IO [FilePath]\n",
    "saveMnist rootDir urlBase keyFiles manager = do\n",
    "    isDir <- doesDirectoryExist rootDir\n",
    "    _ <- if isDir then return () else createDirectory rootDir\n",
    "    mapM saveFile keyFiles\n",
    "    where\n",
    "    saveFile (key, name) = do\n",
    "        let url = urlBase ++ name\n",
    "        let file = rootDir </> key\n",
    "        timestamp $ \"Checking file \" ++ file\n",
    "        e <- doesFileExist file\n",
    "        if e then return () else download manager url >>= (BS.writeFile file)\n",
    "        return file\n",
    "\n",
    "readMnist :: FilePath -> IO (Either MnistLabels MnistImages)\n",
    "readMnist file = do\n",
    "    timestamp $ \"decoding \" ++ file ++ \" ...\"\n",
    "    e <- decodeFileOrFail file\n",
    "    a <- case e of\n",
    "        Left _ -> Left <$> (decodeFile file :: IO MnistLabels)\n",
    "        Right v -> Right <$> (return v :: IO MnistImages)\n",
    "    timestamp $ \"decoded \" ++ file\n",
    "    print \"\"\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSimple :: MnistData -> MnistData -> Int\n",
    "trainingSimple trainers tests = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newManager defaultManagerSettings >>= saveMnist \"mnist\" \"http://yann.lecun.com/exdb/mnist/\" [\n",
    "    (\"train_img\", \"train-images-idx3-ubyte.gz\")\n",
    "  , (\"train_label\", \"train-labels-idx1-ubyte.gz\")\n",
    "  , (\"test_img\", \"t10k-images-idx3-ubyte.gz\")\n",
    "  , (\"test_label\", \"t10k-labels-idx1-ubyte.gz\")\n",
    "  ] >>= mapM readMnist >>= \\ms -> do\n",
    "      let [trainers, tests] = map (\\[Right a, Left b] -> b `zipMnist` a) $ chunksOf 2 ms\n",
    "      let result = trainingSimple trainers tests\n",
    "      print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Haskell",
   "language": "haskell",
   "name": "haskell"
  },
  "language_info": {
   "codemirror_mode": "ihaskell",
   "file_extension": ".hs",
   "name": "haskell",
   "pygments_lexer": "Haskell",
   "version": "8.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
